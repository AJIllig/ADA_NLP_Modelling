{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applied Data Analytics: Assessment 2\n",
    "Alexis Illig  \n",
    "Student No: 29342872  \n",
    "\n",
    "Date: 26/11/2018  \n",
    "Enviroment: R v3.5.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(tidyr)\n",
    "library(data.table)\n",
    "library(lattice)\n",
    "library(ggplot2)\n",
    "library(scales)    # number formatting\n",
    "library(caret)     # confusion matrix\n",
    "library(LiblineaR) # svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User-defined Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to display the accuracy and macro-f1 score\n",
    "metrics <- function(actual, predicted){\n",
    "    df <- cbind(as.data.frame(actual), as.data.frame(predicted))\n",
    "    names(df) <- c('Actual', 'Predicted')\n",
    "    conf_m <- confusionMatrix(data = factor(df$Predicted, levels = as.character(seq(1:23))),\n",
    "                               reference = factor(df$Actual, ordered = TRUE))\n",
    "    # metrics\n",
    "    tot_acc <- percent(conf_m$overall['Accuracy'])\n",
    "    macro_f1 <- percent(mean(conf_m$byClass[,'F1']))\n",
    "\n",
    "    # view metrics\n",
    "    paste('Accuracy: ',tot_acc,'   ', 'Macro-F1: ', macro_f1)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Feature Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the feature vectors into datatables\n",
    "train <- fread('cleaned_training_vectors.csv', header=TRUE, drop=c(1))\n",
    "test <- fread('cleaned_testing_vectors.csv', header=TRUE, stringsAsFactors=FALSE, drop=c(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Training and Validation Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the datasets\n",
    "set.seed(42)\n",
    "train <- train[sample(nrow(train)),]\n",
    "test <- test[sample(nrow(test)),]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partition\n",
    "set.seed(42)\n",
    "split_inds <- sample(nrow(train), nrow(train)*(0.8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train and validation sets\n",
    "x_train <- as.matrix(train[split_inds, -c(1)])\n",
    "y_train <- as.matrix(train$Category[split_inds])\n",
    "x_val <- as.matrix(train[-split_inds, -c(1)])\n",
    "y_val <- as.matrix(train$Category[-split_inds])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model - Linear SVM\n",
    "The tuning cell is based on an example provided in the LiblineaR documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for C=0.225 : 0.7727134 accuracy\n",
      "Best model type is: 2 \n",
      "Best cost is: 0.225 \n",
      "Best training accuracy is: 0.7727134 \n",
      "Time difference of 0.9173293 mins\n"
     ]
    }
   ],
   "source": [
    "t1 <- Sys.time()\n",
    "\n",
    "#  set tuning ranges (toggled for assessment submission)\n",
    "k <- 5                                              # K-fold cross validation\n",
    "tryTypes <- c(2)                                    # 0 for logistic regression, 2 for l2-l2 linear svm\n",
    "# tryCosts <- c(.01, .1, 1, 10, 100, 1000, 2000)    # cost tuning for logistic regression\n",
    "# tryCosts <- c(c(0.001,0.005,0.01, 0.05), \n",
    "#               seq(0.1, 0.5, by=0.05), \n",
    "#               c(1, 5, 10, 50, 100, 500, 1000))      # cost tuning for linear svm\n",
    "\n",
    "# set tuned parameters (toggled for assessment submission)\n",
    "tryCosts <- c(0.225)                                # tune cost parameter\n",
    "bestCost <- NA\n",
    "bestAcc <- 0\n",
    "bestType <- NA\n",
    "tuning <- data.table(Type=integer(), Cost=numeric(), Accuracy=numeric())    # repository for tuning parameters\n",
    "\n",
    "# Find the best model with the best cost parameter via k-fold cross-validation\n",
    "counter = 1\n",
    "for(ty in tryTypes){\n",
    "    for(co in tryCosts){\n",
    "#         print(Sys.time())\n",
    "#         print(counter)\n",
    "#         flush.console()\n",
    "        svm_fit <- LiblineaR(data=x_train, target=y_train,\n",
    "                             type=ty, cost=co, cross=k)\n",
    "        tuning <- rbind(tuning, data.table(Type = ty, Cost = co, Accuracy = svm_fit))\n",
    "        cat(\"Results for C=\", co, \" : \", svm_fit, \" accuracy\\n\", sep=\"\")\n",
    "        if(svm_fit > bestAcc){\n",
    "            bestCost <- co\n",
    "            bestAcc <- svm_fit\n",
    "            bestType <- ty\n",
    "        }\n",
    "        counter = counter + 1\n",
    "    }\n",
    "}\n",
    "\n",
    "cat(\"Best model type is:\", bestType,\"\\n\")\n",
    "cat(\"Best cost is:\", bestCost,\"\\n\")\n",
    "cat(\"Best training accuracy is:\", bestAcc,\"\\n\")\n",
    "\n",
    "print(difftime(Sys.time(), t1, units = 'min'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-train best model with best cost value\n",
    "best <- LiblineaR(data=x_train, target=y_train, type=bestType, cost=bestCost, bias=1, verbose=FALSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the validation set classifications\n",
    "svm_preds <- predict(best, x_val, proba=FALSE, decisionValues=TRUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span style=white-space:pre-wrap>'Accuracy:  77.9%     Macro-F1:  76.8%'</span>"
      ],
      "text/latex": [
       "'Accuracy:  77.9\\%     Macro-F1:  76.8\\%'"
      ],
      "text/markdown": [
       "<span style=white-space:pre-wrap>'Accuracy:  77.9%     Macro-F1:  76.8%'</span>"
      ],
      "text/plain": [
       "[1] \"Accuracy:  77.9%     Macro-F1:  76.8%\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Confusion matrix\n",
    "metrics(y_val, svm_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuning Plot\n",
    "An image of this plot is in the Report.  \n",
    "Toggled off for Assessment.  The tuned value is used instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set plot width and height\n",
    "# options(repr.plot.width = 8, repr.plot.height = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the parameter tuning\n",
    "# print(ggplot(data = tuning[1:17], mapping = aes(x = Cost, y = Accuracy)) +  \n",
    "#       geom_line(aes(x = Cost, y = Accuracy)) + geom_point(alpha = 1.0, col = 'blue', size = 1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the test set classes\n",
    "test_preds <- predict(best, test[,-c(1)], proba=FALSE, decisionValues=TRUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe of document names and predictions\n",
    "df <- data.frame(test$Doc, test_preds$predictions)\n",
    "names(df) <- c('Doc', 'Predicted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append 'C' to the classifications\n",
    "df$Predicted <- sub('^', 'C', df$Predicted )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Order by document name\n",
    "df <- df[order(as.numeric(gsub(\"[^[:digit:]]\", \"\", df$Doc))),] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>Doc</th><th scope=col>Predicted</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>6189</th><td>te_doc_1</td><td>C5      </td></tr>\n",
       "\t<tr><th scope=row>23216</th><td>te_doc_2</td><td>C17     </td></tr>\n",
       "\t<tr><th scope=row>10249</th><td>te_doc_3</td><td>C22     </td></tr>\n",
       "\t<tr><th scope=row>23188</th><td>te_doc_4</td><td>C12     </td></tr>\n",
       "\t<tr><th scope=row>24157</th><td>te_doc_5</td><td>C1      </td></tr>\n",
       "\t<tr><th scope=row>2957</th><td>te_doc_6</td><td>C20     </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ll}\n",
       "  & Doc & Predicted\\\\\n",
       "\\hline\n",
       "\t6189 & te\\_doc\\_1 & C5          \\\\\n",
       "\t23216 & te\\_doc\\_2 & C17         \\\\\n",
       "\t10249 & te\\_doc\\_3 & C22         \\\\\n",
       "\t23188 & te\\_doc\\_4 & C12         \\\\\n",
       "\t24157 & te\\_doc\\_5 & C1          \\\\\n",
       "\t2957 & te\\_doc\\_6 & C20         \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | Doc | Predicted | \n",
       "|---|---|---|---|---|---|\n",
       "| 6189 | te_doc_1 | C5       | \n",
       "| 23216 | te_doc_2 | C17      | \n",
       "| 10249 | te_doc_3 | C22      | \n",
       "| 23188 | te_doc_4 | C12      | \n",
       "| 24157 | te_doc_5 | C1       | \n",
       "| 2957 | te_doc_6 | C20      | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "      Doc      Predicted\n",
       "6189  te_doc_1 C5       \n",
       "23216 te_doc_2 C17      \n",
       "10249 te_doc_3 C22      \n",
       "23188 te_doc_4 C12      \n",
       "24157 te_doc_5 C1       \n",
       "2957  te_doc_6 C20      "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "head(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Test Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "write.table(df, 'testing_labels_pred.txt', quote=FALSE, row.names=FALSE, col.names=FALSE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
