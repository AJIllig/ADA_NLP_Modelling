{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applied Analysis Assignment 2: PreProcessing\n",
    "\n",
    "#### Enviroment: Python 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# General\n",
    "from pathlib import Path\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from time import perf_counter    # cell runtime\n",
    "from natsort import natsorted    # intuitive sorting and renaming of variables\n",
    "from itertools import chain      # used in concatenation function\n",
    "# import winsound                  # play a sound (windows only)\n",
    "\n",
    "# Tokenizing\n",
    "import nltk.data                                                                    # punkt sentence tokenizer\n",
    "from nltk.tokenize import PunktSentenceTokenizer, RegexpTokenizer, word_tokenize    # word tokenizers\n",
    "from nltk.corpus import wordnet                                                     # parts of speech tagging\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer                              # lemming and stemming\n",
    "\n",
    "# Stopword Removal\n",
    "import string                        # list of punctuation\n",
    "from nltk.corpus import stopwords    # use nltk short list of stopwords (unused)\n",
    "from nltk.probability import *       # token frequency distributions (FreqDist())\n",
    "\n",
    "# Vectorization\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start timer\n",
    "t_start = perf_counter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Defined Functions\n",
    "See Tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def list_concat(a_list):\n",
    "    '''\n",
    "    concatenates the values in a list\n",
    "    '''\n",
    "    return list(chain.from_iterable(a_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Text Files into Newline separated Lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and Test Docs Location\n",
    "target_folder = Path.home().joinpath('DropBox', \n",
    "                                     'Monash_Uni', \n",
    "                                     'FIT5149 Applied Data Analysis', \n",
    "                                     'Assessment 2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the training documents\n",
    "target_file = target_folder.joinpath('training_docs.txt')\n",
    "\n",
    "with open(target_file, mode='r', encoding='utf-8') as data_file:\n",
    "    tr_docs = data_file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the training class labels\n",
    "target_file = target_folder.joinpath('training_labels_final.txt')\n",
    "\n",
    "with open(target_file, encoding='utf-8') as data_file:\n",
    "    tr_labs = data_file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the test documents\n",
    "target_file = target_folder.joinpath('testing_docs_shuffle.txt')\n",
    "\n",
    "with open(target_file, encoding='utf-8') as data_file:\n",
    "    test_docs = data_file.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Document Dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train: Create a dictionary of document id : content text\n",
    "tr_docs_dict = {}\n",
    "for line in tr_docs:\n",
    "    line = line.strip()\n",
    "    if (line != '' and line.lower() != 'eod'):            # exclude empty and end-of-document lines\n",
    "        if bool(re.match(r'^(ID)', line)):                # identify lines starting with 'ID'\n",
    "            id_k = re.findall(r'(?<=ID ).*$', line)[0]    # get the document id\n",
    "            text = []                                     # empty list to hold document contents\n",
    "        else:\n",
    "            t = re.findall(r'(?<=TEXT ).*$', line)[0]     # get the document contents past the 'TEXT' signifier\n",
    "            text.append(t.lower())                        # normalise to lowercase\n",
    "    tr_docs_dict[id_k] = text                             # create a dictionary of document id key : document content list value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train: Create a dictionary of document id : class label\n",
    "tr_labs_dict = {}\n",
    "for line in tr_labs:\n",
    "    line = line.strip()\n",
    "    id_k = re.findall('(?:\\S+)+', line)    # list the terms\n",
    "    tr_labs_dict[id_k[0]] = id_k[1]        # create dictionary of document id : class label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test: Create a dictionary of document id : content text\n",
    "test_docs_dict = {}\n",
    "for line in test_docs:\n",
    "    line = line.strip()\n",
    "    if (line != '' and line.lower() != 'eod'):            # exclude empty and end-of-document lines\n",
    "        if bool(re.match(r'^(ID)', line)):                # identify lines starting with 'ID'\n",
    "            id_k = re.findall(r'(?<=ID ).*$', line)[0]    # get the document id\n",
    "            text = []                                     # empty list to hold document contents\n",
    "        else:\n",
    "            t = re.findall(r'(?<=TEXT ).*$', line)[0]     # get the document contents past the 'TEXT' signifier\n",
    "            text.append(t.lower())                        # normalise to lowercase\n",
    "    test_docs_dict[id_k] = text                             # create a dictionary of document id key : document content list value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame.from_dict(test_docs_dict, orient='index', columns=['Contents'])\n",
    "test_df.reset_index(inplace=True)\n",
    "test_df.rename(columns = {'index':'Doc'}, inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Doc</th>\n",
       "      <th>Contents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26605</th>\n",
       "      <td>te_doc_26606</td>\n",
       "      <td>the queensland resources council (qrc) says th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26606</th>\n",
       "      <td>te_doc_26607</td>\n",
       "      <td>at least 60 people have died and 4 million hav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26607</th>\n",
       "      <td>te_doc_26608</td>\n",
       "      <td>a campaign to put an end to tailgating by truc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26608</th>\n",
       "      <td>te_doc_26609</td>\n",
       "      <td>there's been more grim reading on the state of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26609</th>\n",
       "      <td>te_doc_26610</td>\n",
       "      <td>an explosion at the indonesian embassy in pari...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Doc                                           Contents\n",
       "26605  te_doc_26606  the queensland resources council (qrc) says th...\n",
       "26606  te_doc_26607  at least 60 people have died and 4 million hav...\n",
       "26607  te_doc_26608  a campaign to put an end to tailgating by truc...\n",
       "26608  te_doc_26609  there's been more grim reading on the state of...\n",
       "26609  te_doc_26610  an explosion at the indonesian embassy in pari..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training labels dataframe\n",
    "tr_labs_df = pd.DataFrame.from_dict(tr_labs_dict, orient='index', columns=['Class'])\n",
    "\n",
    "# Training contents dataframe\n",
    "tr_docs_df = pd.DataFrame.from_dict(tr_docs_dict, orient='index', columns=['Contents'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined dataframe\n",
    "tr_df = pd.concat([tr_labs_df, tr_docs_df], axis=1).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add category column\n",
    "tr_df['Category'] = tr_df['Class'].apply(lambda x: int(x[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename and Rearrange the columns\n",
    "tr_df.rename(columns = {'index':'Doc'}, inplace=True)   \n",
    "cols = tr_df.columns.tolist()\n",
    "cols.insert(1, cols.pop(cols.index('Category')))\n",
    "tr_df = tr_df.reindex(columns= cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Doc</th>\n",
       "      <th>Category</th>\n",
       "      <th>Class</th>\n",
       "      <th>Contents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>106440</th>\n",
       "      <td>tr_doc_106441</td>\n",
       "      <td>23</td>\n",
       "      <td>C23</td>\n",
       "      <td>japan's new ambassador to china has urged stro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106441</th>\n",
       "      <td>tr_doc_106442</td>\n",
       "      <td>23</td>\n",
       "      <td>C23</td>\n",
       "      <td>a man in northern china has driven a car carry...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106442</th>\n",
       "      <td>tr_doc_106443</td>\n",
       "      <td>23</td>\n",
       "      <td>C23</td>\n",
       "      <td>chinese police have rescued 89 children and ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106443</th>\n",
       "      <td>tr_doc_106444</td>\n",
       "      <td>23</td>\n",
       "      <td>C23</td>\n",
       "      <td>turkish energy minister suggests 'far eastern ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106444</th>\n",
       "      <td>tr_doc_106445</td>\n",
       "      <td>23</td>\n",
       "      <td>C23</td>\n",
       "      <td>huey fern tay looks back on some of the news h...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Doc  Category Class  \\\n",
       "106440  tr_doc_106441        23   C23   \n",
       "106441  tr_doc_106442        23   C23   \n",
       "106442  tr_doc_106443        23   C23   \n",
       "106443  tr_doc_106444        23   C23   \n",
       "106444  tr_doc_106445        23   C23   \n",
       "\n",
       "                                                 Contents  \n",
       "106440  japan's new ambassador to china has urged stro...  \n",
       "106441  a man in northern china has driven a car carry...  \n",
       "106442  chinese police have rescued 89 children and ar...  \n",
       "106443  turkish energy minister suggests 'far eastern ...  \n",
       "106444  huey fern tay looks back on some of the news h...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of training documents (needed later for getting percentage of removed empty docs)\n",
    "num_tr_docs = tr_df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize the Training Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Punkt sentence tokeniser\n",
    "sent_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "# sent_tokenizer = PunktSentenceTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to convert POS tags to wordnet tags\n",
    "# The code in this cell is adapted from the following website\n",
    "# http://stackoverflow.com/questions/15586721/wordnet-lemmatization-and-pos-tagging-in-python\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WordNet lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate tokenisation function\n",
    "def bespoke_tokenizer(text):\n",
    "    # identify sentences\n",
    "    sents = sent_tokenizer.tokenize(text.strip())\n",
    "    # tokenize the sentences\n",
    "    toks = [word_tokenize(i) for i in sents] \n",
    "    # tag parts of speech\n",
    "    tb_pos = list_concat([nltk.pos_tag(j) for j in toks])\n",
    "    # convert treebank tags to wordnet tags\n",
    "    wd_pos  = [(pair[0], get_wordnet_pos(pair[1])) for pair in tb_pos]\n",
    "    # derive lemmas\n",
    "    lems = [(lemmatizer.lemmatize(pair[0]), pair[1]) for pair in wd_pos]\n",
    "    # return tokens\n",
    "    return [k[0] for k in lems]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add empty column to dataframes\n",
    "tr_df['Tokens'] = ''\n",
    "test_df['Tokens'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to run the tokenizer and print out run checks\n",
    "def run_tokenizer(df):\n",
    "    # get run time\n",
    "    t0 = perf_counter()  \n",
    "    # tokenize\n",
    "    for item in df.itertuples():\n",
    "        df.at[item.Index, 'Tokens'] = bespoke_tokenizer(item.Contents)\n",
    "        if (item[0] % 1000 == 0):\n",
    "            t1 = perf_counter()\n",
    "            elapsed_time = t1 - t0   \n",
    "            print('Still running! It has been %.1f mins' % ((elapsed_time)/60))\n",
    "    t1 = perf_counter()\n",
    "    elapsed_time = t1 - t0   \n",
    "    print('Total Elapsed time: %.1f mins' % ((elapsed_time)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the dataframe - only run this to recreate the pickle file if needed -  warning: long run time\n",
    "# run_tokenizer(tr_df)      3 ~204 min\n",
    "# run_tokenizer(test_df)    # ~50 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the better way to run the tokenizer but doesn't include print statements of periodioc runtime\n",
    "# %%time\n",
    "# tr_df['Tokens'] = tr_df['Contents'].apply(bespoke_tokenizer)\n",
    "# test_df['Tokens'] = test_df['Contents'].apply(bespoke_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save cleaned and tokenized dataframes to pickle (since running the tokenization takes so long)\n",
    "# tr_df.to_pickle('tokenised_tr_df.pkl')\n",
    "# test_df.to_pickle('tokenised_test_df.pkl')\n",
    "\n",
    "# read the pickled dataframes\n",
    "tr_df = pd.read_pickle(target_folder.joinpath('tokenised_tr_df.pkl'))\n",
    "test_df = pd.read_pickle(target_folder.joinpath('tokenised_test_df.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Doc</th>\n",
       "      <th>Contents</th>\n",
       "      <th>Tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>te_doc_1</td>\n",
       "      <td>mahela jayawardene's sri lanka will leapfrog f...</td>\n",
       "      <td>[mahela, jayawardene, 's, sri, lanka, will, le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>te_doc_2</td>\n",
       "      <td>floodwaters from queensland's central west hav...</td>\n",
       "      <td>[floodwaters, from, queensland, 's, central, w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>te_doc_3</td>\n",
       "      <td>a circus trainer is fighting for his life afte...</td>\n",
       "      <td>[a, circus, trainer, is, fighting, for, his, l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>te_doc_4</td>\n",
       "      <td>forget wimbledon for now, fed cup captain davi...</td>\n",
       "      <td>[forget, wimbledon, for, now, ,, fed, cup, cap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>te_doc_5</td>\n",
       "      <td>two women have been charged after an investiga...</td>\n",
       "      <td>[two, woman, have, been, charged, after, an, i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Doc                                           Contents  \\\n",
       "0  te_doc_1  mahela jayawardene's sri lanka will leapfrog f...   \n",
       "1  te_doc_2  floodwaters from queensland's central west hav...   \n",
       "2  te_doc_3  a circus trainer is fighting for his life afte...   \n",
       "3  te_doc_4  forget wimbledon for now, fed cup captain davi...   \n",
       "4  te_doc_5  two women have been charged after an investiga...   \n",
       "\n",
       "                                              Tokens  \n",
       "0  [mahela, jayawardene, 's, sri, lanka, will, le...  \n",
       "1  [floodwaters, from, queensland, 's, central, w...  \n",
       "2  [a, circus, trainer, is, fighting, for, his, l...  \n",
       "3  [forget, wimbledon, for, now, ,, fed, cup, cap...  \n",
       "4  [two, woman, have, been, charged, after, an, i...  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Doc</th>\n",
       "      <th>Category</th>\n",
       "      <th>Class</th>\n",
       "      <th>Contents</th>\n",
       "      <th>Tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tr_doc_1</td>\n",
       "      <td>1</td>\n",
       "      <td>C1</td>\n",
       "      <td>two german tourists have been found safe and w...</td>\n",
       "      <td>[two, german, tourist, have, been, found, safe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tr_doc_2</td>\n",
       "      <td>1</td>\n",
       "      <td>C1</td>\n",
       "      <td>act police have seized a rare drug during a ra...</td>\n",
       "      <td>[act, police, have, seized, a, rare, drug, dur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tr_doc_3</td>\n",
       "      <td>1</td>\n",
       "      <td>C1</td>\n",
       "      <td>a 50-year-old brisbane man has been charged wi...</td>\n",
       "      <td>[a, 50-year-old, brisbane, man, ha, been, char...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tr_doc_4</td>\n",
       "      <td>1</td>\n",
       "      <td>C1</td>\n",
       "      <td>in-depth discussions are continuing to resolve...</td>\n",
       "      <td>[in-depth, discussion, are, continuing, to, re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tr_doc_5</td>\n",
       "      <td>1</td>\n",
       "      <td>C1</td>\n",
       "      <td>homicide detectives are still questioning a ma...</td>\n",
       "      <td>[homicide, detective, are, still, questioning,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Doc  Category Class  \\\n",
       "0  tr_doc_1         1    C1   \n",
       "1  tr_doc_2         1    C1   \n",
       "2  tr_doc_3         1    C1   \n",
       "3  tr_doc_4         1    C1   \n",
       "4  tr_doc_5         1    C1   \n",
       "\n",
       "                                            Contents  \\\n",
       "0  two german tourists have been found safe and w...   \n",
       "1  act police have seized a rare drug during a ra...   \n",
       "2  a 50-year-old brisbane man has been charged wi...   \n",
       "3  in-depth discussions are continuing to resolve...   \n",
       "4  homicide detectives are still questioning a ma...   \n",
       "\n",
       "                                              Tokens  \n",
       "0  [two, german, tourist, have, been, found, safe...  \n",
       "1  [act, police, have, seized, a, rare, drug, dur...  \n",
       "2  [a, 50-year-old, brisbane, man, ha, been, char...  \n",
       "3  [in-depth, discussion, are, continuing, to, re...  \n",
       "4  [homicide, detective, are, still, questioning,...  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Stopwords, Hapaxes, and Empty Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Set of Freestanding Punctuation, Contractions, and Common Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define list of punctuation to remove\n",
    "punct = set(string.punctuation)\n",
    "punct.update([\"''\",\"``\",\"...\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get set of very common stopwords\n",
    "# csw = set(stopwords.words(\"english\"))    # nltk's list of stopwords (short)\n",
    "csw = []\n",
    "with open(target_folder.joinpath('stopwords_en.txt')) as f:\n",
    "    csw = set(f.read().splitlines()) # list of stopwords (long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create set of contraction words\n",
    "target_file = target_folder.joinpath('wiki_list_contractions.txt')\n",
    "\n",
    "with open(target_file, encoding='utf-8') as data_file:\n",
    "    wlc = data_file.read()\n",
    "\n",
    "tokenizer = RegexpTokenizer(r\"\\s+\", gaps=True) # (\\s means any whitespace character (\\t\\n\\r\\f\\v))\n",
    "contractions = set(tokenizer.tokenize(wlc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get set of tokens in the test dataset taht are not in the training dataset\n",
    "all_tr_tokens = set(list_concat(tr_df['Tokens'].tolist()))\n",
    "all_test_tokens = set(list_concat(test_df['Tokens'].tolist()))\n",
    "diff = all_test_tokens.difference(all_tr_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional stopwords\n",
    "other = {'ha', 'wa'}    # lemmatizer converts 'has' to 'ha' and 'was' to 'wa'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete set of stopwords\n",
    "stopwords_1 = csw.union(contractions, punct, diff, other)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to keep an item in a list if it is not found in a reference\n",
    "def remove_stopwords(text, stopwords):\n",
    "    return [token for token in text if token not in stopwords]    # list and set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Remove stopwords\n",
    "tr_df['Tokens'] = tr_df['Tokens'].apply(remove_stopwords, args=(stopwords_1,))\n",
    "test_df['Tokens'] = test_df['Tokens'].apply(remove_stopwords, args=(stopwords_1,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming\n",
    "* stem to target all the words missed by the lemmatizer \n",
    "* even if the stemmed word is wrong (ex. bushfires and bushfire $\\rightarrow$ bushfir) it shouldn't matter as long as it's consistent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define stemmer\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Stem\n",
    "tr_df['Tokens'] = tr_df['Tokens'].apply(lambda x : [ps.stem(y) for y in x])\n",
    "test_df['Tokens'] = test_df['Tokens'].apply(lambda x : [ps.stem(y) for y in x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Hapaxes\n",
    "Reduces vectorization time significantly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of all tokens in the corpus\n",
    "all_words = list_concat(tr_df['Tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get frequency distribution of tokens\n",
    "freq_dist = FreqDist(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get hapaxes\n",
    "haps = set(freq_dist.hapaxes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove hapaxes\n",
    "tr_df['Tokens'] = tr_df['Tokens'].apply(remove_stopwords, args=(haps,))\n",
    "test_df['Tokens'] = test_df['Tokens'].apply(remove_stopwords, args=(haps,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(haps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Empty Docs from the Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove docs with k tokens\n",
    "k = 0\n",
    "tr_df = tr_df[tr_df['Tokens'].map(lambda d: len(d)) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage of docs removed\n",
    "'{}%'.format(round(100*(num_tr_docs-tr_df.shape[0])/num_tr_docs, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set of removed indices\n",
    "rm_inds = set(range(0,num_tr_docs-1)) - set(tuple(tr_df.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenate Tokens \n",
    "In preparation to feed to the tf-idf vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the tokens for each document\n",
    "tr_df['Final_Tokens'] = tr_df['Tokens'].str.join(' ')\n",
    "test_df['Final_Tokens'] = test_df['Tokens'].str.join(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize the Tokenized Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the vectorizer model\n",
    "vectorizer = TfidfVectorizer()    # min_df=200, ngram_range=(1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the corpus to be vectorized\n",
    "tar_col = 'Final_Tokens'    # use cleaned corpus\n",
    "# tar_col = 'Contents'       # use raw corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define testing and training sets\n",
    "X_train = tr_df[tar_col]       # train text to vectorize\n",
    "y_train = tr_df['Category']    # train target labels\n",
    "X_test = test_df[tar_col]      # test text to vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Create feature vectors\n",
    "vect = vectorizer.fit(X_train)             # learn the vocabulary and fit idfs\n",
    "X_train_vects = vect.transform(X_train)    # transform the training documents into a document-term matrix\n",
    "X_test_vects = vect.transform(X_test)      # transform the test documents into a document-term matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_test_vects.shape, type(X_test_vects))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify Most Correlated Features by Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary of class : category \n",
    "label_to_categ = dict(tr_df[['Class', 'Category']].drop_duplicates().sort_values('Category').values)\n",
    "# Create datarame to hold the correlation results\n",
    "corr_features = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation score for each feature by class\n",
    "for class_label, category in label_to_categ.items():\n",
    "    chi2score = chi2(X_train_vects, y_train == category)[0]     # get correlation scores\n",
    "    wscores = zip(vectorizer.get_feature_names(),chi2score)     # list scores with feature name, instead of feature number\n",
    "    wchi2 = sorted(wscores, key=lambda x:x[1], reverse=True)    # sort the scores in descending order\n",
    "    corr_features[class_label] = list(zip(*wchi2))[0]           # add the sorted list to the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top N correlated features by class\n",
    "n = 200\n",
    "corr_head = corr_features.head(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View most correlated features by class\n",
    "display(corr_head.loc[:, 'C1':'C11'].head(5))\n",
    "display(corr_head.loc[:, 'C12':].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keep Top-Correlated Features Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set of all the top n correlated features\n",
    "top_corr_dict = {c: corr_head[c] for c in corr_head}\n",
    "top_n = set(list_concat(top_corr_dict.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to keep only items found in a reference\n",
    "def keepwords(text, list_of_words):\n",
    "    return [token for token in text if token in list_of_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove features not in the top correlation set\n",
    "tr_df['Tokens'] = tr_df['Tokens'].apply(keepwords, args=(top_n,))\n",
    "test_df['Tokens'] = test_df['Tokens'].apply(keepwords, args=(top_n,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_df['Final_Tokens'] = tr_df['Tokens'].str.join(' ')\n",
    "test_df['Final_Tokens'] = test_df['Tokens'].str.join(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove docs with k tokens\n",
    "k = 0\n",
    "tr_df = tr_df[tr_df['Tokens'].map(lambda d: len(d)) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage of docs removed\n",
    "'{}%'.format(round(100*(num_tr_docs-tr_df.shape[0])/num_tr_docs, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set of removed indices\n",
    "rm_inds = set(range(0,num_tr_docs-1)) - set(tuple(tr_df.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get pandas series of training indices for mapping after re-vectorizaton\n",
    "tr_df.reset_index(inplace=True, drop=True)\n",
    "tr_index = pd.Series(tr_df['Category'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-Vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define testing and training sets\n",
    "X_train = tr_df[tar_col]       # train text to vectorize\n",
    "y_train = tr_df['Category']    # train target labels\n",
    "X_test = test_df[tar_col]      # test text to vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Create feature vectors\n",
    "vect = vectorizer.fit(X_train)             # learn the vocabulary and fit idfs\n",
    "X_train_vects = vect.transform(X_train)    # transform the training documents into a document-term matrix\n",
    "X_test_vects = vect.transform(X_test)      # transform the test documents into a document-term matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_vects.shape, type(X_train_vects))\n",
    "print(X_test_vects.shape, type(X_test_vects))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA Dimension Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scale\n",
    "(Don't scale tfidf vectors. they are already scaled.)  \n",
    "Scale the feature vectors to mean=0 and unit variance  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the scipy sparse matrices to sparse dataframes \n",
    "# note: this produces NaNs instead of zeroes, which the scaler below doesn't like\n",
    "# train_ftrs = pd.SparseDataFrame(X_train_vects)\n",
    "# test_ftrs = pd.SparseDataFrame(X_test_vects)\n",
    "\n",
    "# convert the scipy sparse matrices to dense dataframes\n",
    "train_temp = pd.DataFrame(X_train_vects.todense())\n",
    "test_temp = pd.DataFrame(X_test_vects.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_temp.shape)\n",
    "print(test_temp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the scaler (mean of 0, variance of 1)\n",
    "# scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit scaler on training set\n",
    "# scaler.fit(train_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Scale both the training set and test sets\n",
    "# train_scaled = scaler.transform(train_temp)\n",
    "# test_scaled = scaler.transform(test_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply PCA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the retained variance (PCA(.95): 95% of the variance is retained)\n",
    "# pca = PCA(.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# Fit PCA on training set\n",
    "# pca.fit(train_ftrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the number of principal compenent vectors (new number of features)\n",
    "# print('number of pca features: ', pca.n_components_)\n",
    "# print('feature reduction of {:.2%}.'.format(pca.n_components_ / train_temp.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # Apply PCA to both the training and test sets\n",
    "# train_ftrs = pca.transform(train_ftrs)\n",
    "# test_ftrs = pca.transform(test_ftrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Note: by default, cross_val_score calls either KFold or StratifiedKFold, both of which have shuffle=False,\n",
    "# so the folds are not random. Since my data is arranged by category, I was trying to predict a lot of classes that\n",
    "# the training set had not seen before, and getting worse results. Using ShuffleSplit remedies this by providing\n",
    "# a truly random set.\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "models = [\n",
    "#     RandomForestClassifier(n_estimators=100, random_state=0),\n",
    "#     LinearSVC(),\n",
    "#     MultinomialNB(),\n",
    "    LogisticRegression(random_state=0, solver = 'lbfgs')    # default solver is incredibly slow which is why it was changed to 'lbfgs'\n",
    "]\n",
    "\n",
    "n_cv = 5\n",
    "CV = ShuffleSplit(n_splits=n_cv, test_size=0.3, random_state=0)\n",
    "cv_df = pd.DataFrame(index=range(n_cv * len(models)))\n",
    "entries = []\n",
    "\n",
    "for m in models:\n",
    "    model_name = m.__class__.__name__\n",
    "#     accuracies = cross_val_score(m, train_ftrs, y_train, scoring='accuracy', cv=CV)\n",
    "    accuracies = cross_val_score(m, X_train_vects, y_train, scoring='accuracy', cv=CV)\n",
    "    for fold_idx, value1 in enumerate(accuracies):\n",
    "        entries.append((model_name, fold_idx, value1))\n",
    "cv_df = pd.DataFrame(entries, columns=['model_name', 'fold_idx', 'accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training accuracy by model\n",
    "print('top', n, 'features, total features =', X_train_vects.shape)\n",
    "cv_df.groupby('model_name').accuracy.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# play sound\n",
    "# winsound.PlaySound('CRCHBELL.WAV', winsound.SND_ASYNC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_end = perf_counter()\n",
    "elapsed_time = t_end - t_start\n",
    "print('Total Elapsed time: %.1f mins' % ((elapsed_time)/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Feature Vectors to File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ftrs = pd.concat([tr_df['Category'], train_temp], axis = 1)\n",
    "test_ftrs = pd.concat([test_df['Doc'], test_temp], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ftrs.to_csv('cleaned_training_vectors.csv')\n",
    "test_ftrs.to_csv('cleaned_testing_vectors.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_end = perf_counter()\n",
    "elapsed_time = t_end - t_start\n",
    "print('Total Elapsed time: %.1f mins' % ((elapsed_time)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# play sound\n",
    "# winsound.PlaySound('CRCHBELL.WAV', winsound.SND_ASYNC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Feature Vector Matrices to Dataframes\n",
    "(this turned out to be a really inefficient way of doing this. pd.DataFrame(..todense()) is much faster.  \n",
    "also, .tocoo() or .row() removed empty rows, which I didn't want.)  \n",
    "in prep for saving to files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert scipy sparse matrix from compressed row format to coordinate format\n",
    "# train_spm = X_train_vects.tocoo()\n",
    "# test_spm = X_test_vects.tocoo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature dataframes\n",
    "# train_ftrs = pd.DataFrame({'Doc':train_spm.row, 'Vocab':train_spm.col, 'TFIDF':train_spm.data, 'Category':0})\n",
    "# test_ftrs = pd.DataFrame({'Doc':test_spm.row, 'Vocab':test_spm.col, 'TFIDF':test_spm.data, 'DocName':''})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# Associate the training features with the corresponding category value\n",
    "# for grp, rows in train_ftrs.groupby('Doc'):\n",
    "#     train_ftrs.loc[rows.index, 'Category'] = tr_df.loc[grp,'Category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# Associate the testing features with the corresponding document name \n",
    "# for grp, rows in test_ftrs.groupby('Doc'):    \n",
    "#     test_ftrs.loc[rows.index, 'DocName'] = test_df.loc[grp,'Doc']\n",
    "    \n",
    "# # play sound\n",
    "# winsound.PlaySound('CRCHBELL.WAV', winsound.SND_ASYNC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add plus one to the Doc values to reflect the document names starting at 1 ('tr/te_doc_1')\n",
    "# train_ftrs['Doc'] = train_ftrs['Doc'].add(1)\n",
    "# test_ftrs['Doc'] = test_ftrs['Doc'].add(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train: Get random rows and check that the category is correct for doc in both dataframes\n",
    "# tr_rand = train_ftrs.sample(n=3)\n",
    "# l = list(tr_rand['Doc'].apply(lambda x: \"{}{}\".format('tr_doc_', str(x))))\n",
    "# print('tr_df')\n",
    "# display(tr_df.query('Doc == @l'))\n",
    "# print('train_ftrs')\n",
    "# display(tr_rand.sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test: Get random rows and check that doc and document name match\n",
    "# test_rand = test_ftrs.sample(n=3)\n",
    "# l = list(test_rand['Doc'].apply(lambda x: \"{}{}\".format('te_doc_', str(x))))\n",
    "# print('test_df')\n",
    "# display(test_df.query('Doc == @l'))\n",
    "# print('test_ftrs')\n",
    "# display(test_rand.sort_index())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
